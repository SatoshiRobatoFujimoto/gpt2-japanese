# gpt2-japanese pretrain models
Japanese GPT2 Generation Pretrained Model

## 学習済みGPT2日本語モデル

- 現在、パラメーター数117Mの一番小さなモデルのみ公開しています
  - より大きなモデルも、学習次第公開してゆく予定です

### 学習させたコーパスについて

[corpus.md](corpus.md)

### 分かち書きの語彙数による違いについて

オリジナルのGPT-2では、出力する語彙数は50256種類とかなり巨大です。

パラメーター数117Mの最小モデルの場合、transformer層の出力は768次元なので、ここから50256クラスを出力するのは、やや無理があるのでは？と感じます。

transformer層の出力をfc層にいれてlogitsにする訳ですが、たぶん、1.5Bパラメーター巨大なモデルに合わせた語彙数になっているのではないか、と予想します。

そこで、117Mパラメーターのモデルでは、255種類の出力で済むバイト単位の学習と、50256種類の出力を利用するワード単位の学習を行い、比較できるようにしました。

本当は、もっと他の語彙数も作って、語彙数によるGPT-2の性能比較とかもしてみたいとは思いますが、リソースが足りないので先延ばしです。こちらは計算マシンを提供してくれる方がいれば、共同研究したいと思っています。

## 公開している学習済みモデル


**v1モデル**

- バイト単位で学習させたモデルです。
    - 分かち書き用のSentencePieceも入っていますが、実は意味ありません（学習させてから気づいた汗）。


- v1モデルは、117Mパラメーターのものを三種類公開しています
  - モデル1：汎用日本語生成モデル
    - https://www.nama.ne.jp/models/ja-117M.tar.bz2
    - 汎用的に使えるモデルです
  - モデル2：小説生成モデル
    - https://www.nama.ne.jp/models/ja-117M_novel.tar.bz2
    - Web小説を学習させたモデルです
  - モデル3：Wikipediaモデル
    - https://www.nama.ne.jp/models/ja-117M_wiki.tar.bz2
    - 最初に公開したモデルです。新しい汎用モデルの方が良いので、基本使いません


**v2モデル**

- ワード単位で学習させたモデルです。
    - SentencePieceでコーパス全体を50000ワードに分かち書きし、&lt;unk&gt;を避けるため追加で255語彙をバイト単位にし、<|endoftext|>も合わせた50256語彙です。

- v2モデルは、117Mパラメーターのものをニ種類公開しています
  - モデル1：汎用日本語生成モデル
    - https://www.nama.ne.jp/models/ja-117M_v2.tar.bz2
    - 汎用的に使えるモデルです
  - モデル2：小説生成モデル
    - https://www.nama.ne.jp/models/ja-117M_novel_v2.tar.bz2
    - Web小説を学習させたモデルです

**v2モデルの方が性能は良いはずなのですが、入力contextがない又は短い場合の文章生成に関しては、v1モデルのほうが安定しているような感じがします。**（top_kとtop_pの設定によります：後述）

**文章のベクトル化（transformer）は多分v2モデルのほうが良いです。**

### top_kとtop_pの設定

オリジナルのGPT-2の論文によると、top_k=40が一番良い結果と報告されています。

実際、バイト単位で学習したv1モデルのときは、top_k=40で良い結果が現れました。

**例：**

```
$ python3 gpt2-generate.py --model ja-117M --top_k 40 --top_p 0

実行例えばデバッグの際の実行コマンドってなにものです。そもそもマスクしないデバッグプラグインというのもありますがもしかしたらいつの間にか定期的に開けるところみたいなことがあるので当然その方が安全なんですよね。
なかなかログアウトできない人が多いですが(;^_^A

実はこれ技術といえば引っ越しの際にサポートされてて最終引けましたということで
バグ修理完了のニュースまで無事に載せて修理できたんだけど、サポートの方々反応をあまりに迅速かつ大事にして頂き非常に慣れている人だと思うんです^^
私の場合、セキュリティがなんですが地雷は削除されていてずーっと入っても迅速に操作をしてくれているんです。
悪質ならこの程度より実行コマンドがどんな弊�
になっても良い行為なんだろうなぁ。(^_^;)
敷居にもなってそれが微妙でした!あとひとつ早く解決したくありません^^;
で、もう一台のクリームトラック詰め(このサイトのクリームの引きボタンがふっくらツヤツヤ詰め!)をすべて引きで巻いておこう!
このクリームと同じで、反応がありますが誤って前後に引っ張られて落下する危険があります。
また、セキュリティも大切です。反応がある時に引っ越し(略)の際にごまかすかも?シェイプにすり替えてれば
ホコリが気になるのか?衛生面?玄関先ですが子供達と話してみてください
なので今のトラックをひっかければ面倒になると思います。
修理の内容に関しては他に良い経験がありますか?

（・・・略）
```


```
$ python3 gpt2-generate.py --model ja-117M --context="坂本俊之は、ウェブをクローリングして作成したコーパスで学習させた、GPT2-Japaneseの新しいモデル2種類をリリースしました。" --top_k 40 --top_p 0

これらの作成全てに、ユーザーは対応した物理学とコンピューターやリンクそしてインタラクティブな操作を定義します。コーパスの使用にはパラメータがあり、ショックケースとクラスの類似モデルが用意されています。ユーザーはプログラミング言語初めてのGPT2をレクチャーします。
ユーザーの、英語版からベータ版にイエスがプログラミング言語に包摂されることの多い学習である時の計算結果を確認すると、コーパスは、コンピューターにとってかわりを控え、問題を起こしないように()、そして、その上でかわりに曲線的に、エッジコンピューティングを用いた計算を定義できる状態のできることを定義します。

「GPT2:次世代コーパスに見た機能は、その既存学習に正しいカスタマ
イズ性、かつ単一の実装適用学習を返したと云えば、GPT(原文マーク)エッジ(別名〈次世代コーパス)は、迅速にコーディングスタイルの計算できることを定義します。」
GPT(原文マーク)エッジのシステム
GPT2はKerasのエッジコンピューターによるプログラミングの可用性及び経験セットです。

GPT(原文マーク)エッジに関する言葉
言わば「モリースピース(<PRICE>)」です。
「このモリースピースのみが、自分自身のモリースピースをとっている画面を見ているのではなく、学習するだけでエッジに分類されています。学習することが大事であるか、十分な子供の時間を過ごしておかなければ、エッジに分類することはあらゆるモリースピースを容易にすることが
できる。」
```

**しかし、ワード単位で学習したv2モデルのでは、top_k=40では、意味不明なワードの繰り返しが多く出力されます。**


**例：**

```
$ python3 gpt2-generate.py --top_k 40 --top_p 0
cdzpfvccd_xdc|>a.h
x
y
x|Y
\
このような状態は、
xx
X=aa
yx
X=ya
y
xxx-z
yx-z|xz、yz|xz|xz|xz|xz|xz
x|y|yx|yz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz
yz|yz|xx|xz|z|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz
.
Xzx|xz|xz|xx|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xx
Xz|yz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xx|xz|xz|xz|x
xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|xz|x

（・・・略）
```

どうやら、transformerのステータスが適切な状態になるまでに、ある程度の入力が必要なようです。

**v2モデルも、contextに長めの文章を入れると、かなり良い感じの出力が得られます。**

ちなみにv1モデルでは、contextが短くてもそこそこ関係性のある出力が得られました。


```
$ python3 gpt2-generate.py --top_k 40 --top_p 0 --context="Cやオペアンプを使わずに、トランジスタなど全て単品（？）の素子で作った電子回路を、「ディスクリート」と言います。
> オーディオアンプでは、自分なりの回路を設計したりすると、どうしても全て自由に設計できるディスクリートで設計したくなるんですね。
> 
> ところが、設計済みの回路図を載せているサイトはあっても（ここもそうだけど）、その設計方法を初心者にも解るように書いているサイトは意外となかったことに気付きました。
> 
> いや、きちんとトランジスタ回路の本買って、自分で勉強しろとそういうことなんですが、今時、何でもネットから情報を仕入れてなんぼの時代。ま、突き進んで行くと、最終的にネットの情報だけでは物足りなくなって、書籍で勉強しなければならなくなるのは確実なんですが、ようは入り方ですよね。
> 所詮趣味でやってるんだし、「とりあえずこうやればアンプを設計できるよ」というのを提示して、とりあえずディスクリートアンプの設計に触れてみてもらう･･･というのもアリかな、と思い、暇な時間をつぶすつもりでつらつらと書いてみました。
"
そんな時にもこういう本がありますよね。
もちろん、私は「アンプ」「電源供給系」という分野も持っています。
例えば・・・
「ネットワークアンプ」(Vector)なら、ネットワーク接続時に電源コードとボリュームを分離する機能を持つ、アナログ用(R)トランジスタの回路図が載っていますよ。アンプの「配線図」と、トランジスタの「トランジスタ」では、構成は同じでも構成が違うわけですから、「2個+2個のボリューム」の配置になっていることになります。
なので、1本の線で繋がれた2つのアンプと1番目の線で繋がれた2つのトランジスタ(電源供給系統など)をつなぐために、それぞれ2本の線をつなぐような回路図が載っているわけです。
実際にデジタル電源供給ネットワークという技術の進歩も、この技術も、アナログ電源供給ネットワークと同じ、アナログ回路図で構成された回路構成の回路となりますからね。
この回路図を参考にして、PCやスピーカーのシステムをスピーカーに、アナログ回路に、それぞれ配線して、ネットワークの分配(出力分配)をし、それぞれアンプの供給端子(ボリューム)で、それぞれ回路を決めていくことができるようになります。
アンプの供給端子とネットワークの分配の2個のアンプを、スピーカーの入力としてつないでいきます。この構成により、各アンプそれぞれに、アンプに各ネットワークの分配(出力分配)が生じ、同時にアンプに回路図も、アンプに電源回路図も、そしてそれぞれのアンプに応じて分配されるわけです。
この回路図は、PCやスピーカーのシステムを構成できるようになっています。例えば、アンプを1個のアンプへ3個接続しているわけですが、そのアンプはPCのネットワークに接続しているわけですね。
アナログ出力端子には、2個のアンプが接続されているわけです。
なので、PC(電源供給電源システム)とスピーカーにはそれぞれ、それぞれ1つのネットワークに2個のアンプと2つの分配があります。
私が「ネットワークアンプ」の入門として書いている「電源供給系統」は、電源供給系統として、アンプに2つのアンプと2つの電源供給経路をつなげてアンプ、スピーカー、とつないでいく、というプログラムです。「電源供給系統」の回路図では2の回路図(電源供給系)

（・・・略）
```

**v2モデルで、contextがない状態、又は短い状態で良い結果を出すには、top_kではなくtop_pを使ってやると良いようです。**

そこで、現在、デフォルトの値を、top_p=1、top_k=0としています。

**v2モデルでも、contextが長い場合は、top_kを使ってやるほうが、関連性が高くかつ自然で良い結果が得られます。**

contextが短くても、top_kをさらに大きな値（4000とか）にすると、ある程度は自然な文章が出ます。ただし、contextが全く無い場合は、top_pを設定しないと、かなりの頻度で不自然な出力になります。ちなみにtop_kとtop_pは排他です。どちらかが0である必要があります。

現時点での、私の試行による仮設としては、**top_pを設定すると、出力の文章はtop_kより自然な文章になるのですが、入力contextとの関連性は少なくなるようです。top_kはcontextの長さと反比例するように設定する必要があって、top_kで指定したほうが、入力contextと関連性のある文章が出力されるようです。**

このあたりの定性化については、後の研究を待つ必要がありそうです。出力の性能を定数化して比較してみても面白いかもしれません。


